
# Test Case Validation & Selector Consistency Enforcement Guide

**This guide is mandatory. The agent MUST strictly follow every step below, in order, for every validation cycle. No step must be skipped, omitted, or reordered. Each step must be completed and verified before proceeding to the next.**

---

## Step 1: Clean Test Case Validation
**Enforcement:**
1. You MUST inspect every test case for an authentication step. Only if authenticatiion is missing, add authentication before any test actions.
2. You MUST remove any broad try/catch blocks around tests or assertions. Only use minimal try/catch when absolutely necessary, and always rethrow or assert on caught errors. Never swallow errors.
3. Replace any control flow based on try/catch with explicit timeouts, retries, or waits provided by the test framework.
4. Search for all log statements (`console.log`, `console.warn`, `console.error`, etc.):
    - Replace warning/error logs with explicit `expect` assertions.
    - Remove all other log statements. No debugging output is allowed in final tests.
5. Ensure every test case contains clear, meaningful assertions. Manual inspection of logs is strictly prohibited.
6. Do not proceed until all above points are verified and fixed.
7. Proceed to ## Step 2: Selector Consistency Review

---

## Step 2: Selector Consistency Review
**Enforcement:**
1. You MUST extract exact **ALL** `data-testid` and `aria-label` values from BOTH the test files and the related application HTML files. For each selector, create a mapping table that lists the corresponding IDs from both sources side by side, so that any differences or mismatches are clearly visible.

3. For each selector in the table:
    - If present in test but not in HTML, You MUST add it to the application code immediately.
    - If present in application but not in test, determine if a test should be added or if the selector is redundant. Take action accordingly.
    - If mismatched, You MUST fix the mismatch in the correct file (You MUST ask me if I want to fix it in the test or application code).
4. Do not proceed until all selectors are fully aligned.
5. Proceed to ## Step 3: Selector Priority Standards

---

## Step 3: Selector Priority Standards
**Enforcement:**
1. You MUST review all selector usage in tests. Prioritize `getByTestId()` over `getByRole()` and `getLocator()`.
2. You MUST convert any usage of `getByRole()` or `getLocator()` to `getByTestId()` wherever possible.
    - **First Choice:** Use `getByTestId()` with unique `data-testid` attributes.
    - **Second Choice (only if necessary):** Use `getByRole()` with distinguishing attributes.
    - **Last Resort:** Use `getLocator()` only if no other option is feasible.
3. Document any exceptions where `getByTestId()` cannot be used, with clear technical justification.
4. Do not proceed until all selector priorities are enforced and exceptions are documented.

---

## Step 4: Fixture & Mocked Data Consistency
**Enforcement:**
1. You MUST review all test fixtures and mocked data for alignment with the actual data rendered by the application.
2. You MUST update fixtures and mocked data to accurately reflect intended test scenarios.
3. You MUST ensure all tests use the updated fixtures and mocked data.
4. Do not proceed until fixture and mocked data consistency is fully verified.
5. Proceed to ## Step 5: Test Execution & Failure Resolution
---

## Step 5: Test Execution & Failure Resolution
**Enforcement:**
1. You MUST run the entire test suite after making any changes to log statements, selectors, fixtures, or mocked data.
2. You MUST analyze all test failures:
    - Identify root causes (e.g., selector mismatches, missing assertions, application changes).
    - Resolve failures by updating tests or application code as required.
3. Repeat test execution until all tests pass and selectors are consistent across both test and application code.
4. After user confirmation, repeat the validation and resolution steps for any additional errors or new requirements.
5. Do not consider the validation cycle complete until all tests pass and all steps above are strictly enforced.

---

**Implementation Alignment:**
If a test case expects a specific behavior or implementation, but the application code does not match, update the application code so its behavior and implementation align with the test requirements. This ensures tests validate intended outcomes and the application delivers expected results. This alignment is mandatory.


